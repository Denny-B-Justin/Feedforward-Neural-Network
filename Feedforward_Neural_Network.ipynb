{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY1aHHzdS3zk"
      },
      "source": [
        "# **Feedforward Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "D1NAE_e07-9a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sigmoid(X, j):\n",
        "    if j==0:\n",
        "        return 1 / (1 + np.exp(-X))\n",
        "    else:\n",
        "        s = sigmoid(X, 0)\n",
        "        return s * (1 - s)\n",
        "\n",
        "def tanh(X, j):\n",
        "    if j==0:\n",
        "        return (np.exp(X) - np.exp(-X)) / (np.exp(X) + np.exp(-X))\n",
        "    else:\n",
        "        return 1 - ((np.exp(X) - np.exp(-X)) / (np.exp(X) + np.exp(-X))) ** 2\n",
        "\n",
        "def relu(X, j):\n",
        "    if j==0:\n",
        "        return np.maximum(0.0, X)\n",
        "    else:\n",
        "        return np.where(X>0, 1.0, 0.0)\n",
        "\n",
        "def leaky_relu(X, j):\n",
        "    if j==0:\n",
        "        return np.maximum(0.1 * X, X)\n",
        "    else:\n",
        "        return np.where(X>0, 1.0, 0.1)\n",
        "\n",
        "def silu(X, j):\n",
        "    if j==0:\n",
        "        return X / (1 + np.exp(-X))\n",
        "    else:\n",
        "        s = 1 / (1 + np.exp(-X))\n",
        "        return s * (1 + X * (1 - s))\n",
        "\n",
        "def linear(X, j):\n",
        "    if j==0:\n",
        "        return X\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "def mean_squared_error(Y1, Y2, j):\n",
        "    if j == 0:\n",
        "        return np.sum((Y1 - Y2) ** 2) / (2 * (Y1.shape[0]))\n",
        "    else:\n",
        "        return (Y1 -Y2) / Y1.shape[0]\n",
        "\n",
        "class FeedForwardNeuralNetwork:\n",
        "    def __init__(self, input_shape, layers, layers_activations):\n",
        "        self.layer_count=len(layers)\n",
        "\n",
        "        self.W = [np.random.normal(0, np.sqrt(2 / input_shape), size=(input_shape, layers[0]))]\n",
        "        self.b = [np.zeros((1, layers[0]))]\n",
        "        if layers_activations[0] == \"silu\":\n",
        "            self.activations = [silu]\n",
        "        elif layers_activations[0] == \"relu\":\n",
        "            self.activations = [relu]\n",
        "        elif layers_activations[0] == \"leaky_relu\":\n",
        "            self.activations = [leaky_relu]\n",
        "        elif layers_activations[0] == \"sigmoid\":\n",
        "            self.activations = [sigmoid]\n",
        "        elif layers_activations[0] == \"tanh\":\n",
        "            self.activations = [tanh]\n",
        "        elif layers_activations[0] == \"linear\":\n",
        "            self.activations = [linear]\n",
        "        else:\n",
        "            self.activations = [silu]\n",
        "            print(\"Error in activation function assignment in layer 1\")\n",
        "\n",
        "        for i in range(1, self.layer_count):\n",
        "            self.W.append(np.random.normal(0, np.sqrt(2 / layers[i - 1]), size=(layers[i - 1], layers[i])))\n",
        "            self.b.append(np.zeros((1, layers[i])))\n",
        "\n",
        "            if layers_activations[i] == \"silu\":\n",
        "                self.activations.append(silu)\n",
        "            elif layers_activations[i] == \"relu\":\n",
        "                self.activations.append(relu)\n",
        "            elif layers_activations[i] == \"leaky_relu\":\n",
        "                self.activations.append(leaky_relu)\n",
        "            elif layers_activations[i] == \"sigmoid\":\n",
        "                self.activations.append(sigmoid)\n",
        "            elif layers_activations[i] == \"tanh\":\n",
        "                self.activations.append(tanh)\n",
        "            elif layers_activations[i] == \"linear\":\n",
        "                self.activations.append(linear)\n",
        "            else:\n",
        "                self.activations.append(silu)\n",
        "                print(\"Error in activation function assignment in layer\", i + 1)\n",
        "\n",
        "    def initialize_nadam(self):\n",
        "        self.m_t=[]\n",
        "        self.v_t=[]\n",
        "\n",
        "        for i in range(self.layer_count):\n",
        "            self.m_t.append(np.zeros(self.W[i].shape))\n",
        "            self.m_t.append(np.zeros(self.b[i].shape))\n",
        "            self.v_t.append(np.zeros(self.W[i].shape))\n",
        "            self.v_t.append(np.zeros(self.b[i].shape))\n",
        "\n",
        "    def update_nadam(self, dW, db):\n",
        "        for i in range(self.layer_count):\n",
        "            self.m_t[2 * i] = self.beta1 * self.m_t[2 * i] + (1 - self.beta1) * dW[self.layer_count - 1 - i]\n",
        "            self.m_t[2 * i + 1] = self.beta1 * self.m_t[2 * i + 1] + (1 - self.beta1) * db[self.layer_count - 1 - i]\n",
        "            self.v_t[2 * i] = self.beta2 * self.v_t[2 * i] + (1 - self.beta2) * (dW[self.layer_count - 1 - i] ** 2)\n",
        "            self.v_t[2 * i + 1] = self.beta2 * self.v_t[2 * i + 1] + (1 - self.beta2) * (db[self.layer_count - 1 - i] ** 2)\n",
        "\n",
        "    def nadam(self, i, j):\n",
        "        return ((self.m_t[2 * i + j] / (1 - self.beta1 ** self.t)) / np.sqrt((self.v_t[2 * i + j] / (1 - self.beta2 ** self.t))+self.epsilon))\n",
        "\n",
        "    def compile_model(self, optimizer, loss, learning_rate, alpha, beta1, beta2, epsilon):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.alpha = alpha\n",
        "\n",
        "        if optimizer == \"nadam\":\n",
        "            self.optimizer = self.nadam\n",
        "            self.update_optimizer = self.update_nadam\n",
        "            self.initialize_optimizer=self.initialize_nadam\n",
        "            self.beta1 = beta1\n",
        "            self.beta2 = beta2\n",
        "            self.epsilon = epsilon\n",
        "\n",
        "        if loss == \"mean_squared\":\n",
        "            self.loss = mean_squared_error\n",
        "\n",
        "    def train(self, X_train, y_train, epochs, batch_size):\n",
        "        self.t = 0\n",
        "        self.error = []\n",
        "        self.initialize_optimizer()\n",
        "\n",
        "        X_train_size = X_train.shape[0]\n",
        "\n",
        "        for p in range(epochs):\n",
        "            permu = np.random.permutation(X_train_size)\n",
        "            pos = 0\n",
        "\n",
        "            while(pos < X_train_size):\n",
        "                self.t += 1\n",
        "\n",
        "                if pos + batch_size <= X_train_size:\n",
        "                    X_mini_batch = X_train[permu[pos : pos + batch_size]]\n",
        "                    y_mini_batch = y_train[permu[pos : pos + batch_size]]\n",
        "                    pos += batch_size\n",
        "\n",
        "                else:\n",
        "                    X_mini_batch = X_train[np.concatenate((permu[pos : X_train_size], permu[0 : batch_size - X_train_size + pos]), axis=0)]\n",
        "                    y_mini_batch = y_train[np.concatenate((permu[pos : X_train_size], permu[0 : batch_size - X_train_size + pos]), axis=0)]\n",
        "                    pos += batch_size\n",
        "\n",
        "                a = [X_mini_batch @ self.W[0] + self.b[0]]\n",
        "                h = [self.activations[0](a[0], 0)]\n",
        "                for i in range(1, self.layer_count):\n",
        "                    a.append(h[i-1] @ self.W[i] + self.b[i])\n",
        "                    h.append(self.activations[i](a[i], 0))\n",
        "\n",
        "                self.error.append(self.loss(h[self.layer_count - 1], y_mini_batch, 0))\n",
        "\n",
        "                dh = [(h[self.layer_count - 1] - y_mini_batch) / batch_size]\n",
        "                da = []\n",
        "                dW = []\n",
        "                db = []\n",
        "                for i in range(self.layer_count - 1, 0, -1):\n",
        "                    da.append(dh[-1] * self.activations[i](a[i], 1))\n",
        "                    dW.append(h[i-1].T @ da[-1])\n",
        "                    db.append(np.sum(da[-1], axis=0, keepdims=True))\n",
        "                    dh.append(da[-1] @ self.W[i].T)\n",
        "                da.append(dh[-1] * self.activations[0](a[0], 1))\n",
        "                dW.append(X_mini_batch.T @ da[-1])\n",
        "                db.append(np.sum(da[-1], axis=0, keepdims=True))\n",
        "\n",
        "                self.update_optimizer(dW, db)\n",
        "\n",
        "                for i in range(self.layer_count):\n",
        "                    self.W[i] -= self.learning_rate * ((self.optimizer(i, 0) + self.alpha * dW[self.layer_count - 1 - i]))\n",
        "                    self.b[i] -= self.learning_rate * ((self.optimizer(i, 1) + self.alpha * db[self.layer_count - 1 - i]))\n",
        "\n",
        "    def predict(self, X_new_data):\n",
        "        a = X_new_data @ self.W[0] + self.b[0]\n",
        "        h = self.activations[0](a, 0)\n",
        "\n",
        "        for i in range(1, self.layer_count):\n",
        "            a = h @ self.W[i] + self.b[i]\n",
        "            h = self.activations[i](a, 0)\n",
        "\n",
        "        return h"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
